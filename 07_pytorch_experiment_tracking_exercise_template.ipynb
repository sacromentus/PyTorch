{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNqPNlYylluR"
   },
   "source": [
    "# 07. PyTorch Experiment Tracking Exercise Template\n",
    "\n",
    "Welcome to the 07. PyTorch Experiment Tracking exercise template notebook.\n",
    "\n",
    "> **Note:** There may be more than one solution to each of the exercises. This notebook only shows one possible example.\n",
    "\n",
    "## Resources\n",
    "\n",
    "1. These exercises/solutions are based on [section 07. PyTorch Transfer Learning](https://www.learnpytorch.io/07_pytorch_experiment_tracking/) of the Learn PyTorch for Deep Learning course by Zero to Mastery.\n",
    "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/cO_r2FYcAjU).\n",
    "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions).\n",
    "\n",
    "> **Note:** The first section of this notebook is dedicated to getting various helper functions and datasets used for the exercises. The exercises start at the heading \"Exercise 1: ...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf8ab9cyHTzU"
   },
   "source": [
    "### Get various imports and helper functions\n",
    "\n",
    "We'll need to make sure we have `torch` v.1.12+ and `torchvision` v0.13+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MOv1De4mxeL",
    "outputId": "a4f9fdd9-f225-4861-e204-e26186365bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.0+cu121\n",
      "torchvision version: 0.16.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "\n",
    "    major = int(torch.__version__.split(\".\")[0])\n",
    "    minor = int(torch.__version__.split(\".\")[1])\n",
    "    assert (major >= 2) or (minor >= 12), \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Nf-DsrZipCE9",
    "outputId": "f4cc7d1c-da78-4eb4-c753-4e135771650c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Make sure we have a GPU\n",
    " device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    " device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i_52puIeoab3"
   },
   "outputs": [],
   "source": [
    "# Get regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DBj8I3P9pNK2"
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6R-CS53pTLS",
    "outputId": "3f7688b7-0b86-4cd8-bb11-4e71f8e7270f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_data(source: str,\n",
    "                  destination: str,\n",
    "                  remove_source: bool = True) -> Path:\n",
    "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
    "\n",
    "    Args:\n",
    "        source (str): A link to a zipped file containing data.\n",
    "        destination (str): A target directory to unzip data to.\n",
    "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
    "\n",
    "    Returns:\n",
    "        pathlib.Path to downloaded data.\n",
    "\n",
    "    Example usage:\n",
    "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                      destination=\"pizza_steak_sushi\")\n",
    "    \"\"\"\n",
    "    # Setup path to data folder\n",
    "    data_path = Path(\"data/\")\n",
    "    image_path = data_path / destination\n",
    "\n",
    "    # If the image folder doesn't exist, download it and prepare it...\n",
    "    if image_path.is_dir():\n",
    "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Download pizza, steak, sushi data\n",
    "        target_file = Path(source).name\n",
    "        with open(data_path / target_file, \"wb\") as f:\n",
    "            request = requests.get(source)\n",
    "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
    "            f.write(request.content)\n",
    "\n",
    "        # Unzip pizza, steak, sushi data\n",
    "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
    "            print(f\"[INFO] Unzipping {target_file} data...\")\n",
    "            zip_ref.extractall(image_path)\n",
    "\n",
    "        # Remove .zip file\n",
    "        if remove_source:\n",
    "            os.remove(data_path / target_file)\n",
    "\n",
    "    return image_path\n",
    "\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BE60IEEkr89l"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str=None):\n",
    "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "\n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0BH4ONGsgNB",
    "outputId": "077f7b39-f4d0-44dd-b74f-8ead04fb7add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/test_experiment_name/this_is_the_model_name/add_a_little_extra_if_you_want...\n"
     ]
    }
   ],
   "source": [
    "# Create a test writer\n",
    "writer = create_writer(experiment_name=\"test_experiment_name\",\n",
    "                       model_name=\"this_is_the_model_name\",\n",
    "                       extra=\"add_a_little_extra_if_you_want\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VwO0Q1eFsusV"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from going_modular.engine import train_step, test_step\n",
    "\n",
    "# Add writer parameter to train()\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device, \n",
    "          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n",
    "          ) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Stores metrics to specified writer log_dir if present.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "      writer: A SummaryWriter() instance to log model results to.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                train_acc: [...],\n",
    "                test_loss: [...],\n",
    "                test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "              {train_loss: [2.0616, 1.0537],\n",
    "                train_acc: [0.3945, 0.3945],\n",
    "                test_loss: [1.2641, 1.5706],\n",
    "                test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.75)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        ### New: Use the writer parameter to track experiments ###\n",
    "        # See if there's a writer, if so, log to it\n",
    "        if writer:\n",
    "            # Add results to SummaryWriter\n",
    "            writer.add_scalars(main_tag=\"Loss\", \n",
    "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                \"test_acc\": test_acc}, \n",
    "                               global_step=epoch)\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "    ### End new ###\n",
    "        \n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh8jKzHYHYL3"
   },
   "source": [
    "### Download data\n",
    "\n",
    "Using the same data from https://www.learnpytorch.io/07_pytorch_experiment_tracking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68QGCR_1tzif",
    "outputId": "a5073b19-1463-4d8a-ec08-5399945196a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
      "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download 10 percent and 20 percent training data (if necessary)\n",
    "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9L2rCRxvt1ED",
    "outputId": "30b8202e-96f3-443f-e89e-2a83e11b1c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory 10%: data/pizza_steak_sushi/train\n",
      "Training directory 20%: data/pizza_steak_sushi_20_percent/train\n",
      "Testing directory: data/pizza_steak_sushi_20_percent/test\n"
     ]
    }
   ],
   "source": [
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
    "test_dir = data_20_percent_path / \"test\"\n",
    "\n",
    "# Check the directories\n",
    "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "K35q9wswt6NH"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create a transform to normalize data distribution to be inline with ImageNet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Create a transform pipeline\n",
    "simple_transform = transforms.Compose([\n",
    "                                       transforms.Resize((224, 224)),\n",
    "                                       transforms.ToTensor(), # get image values between 0 & 1\n",
    "                                       normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBuEla8pHea9"
   },
   "source": [
    "### Turn data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlQU94HBuqOq",
    "outputId": "db4b9144-fd3b-4f31-e0c1-ebd60f906232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 32 in 10 percent training data: 8\n",
      "Number of batches of size 32 in 20 percent training data: 15\n",
      "Number of batches of size 32 in testing data: 8 (all experiments will use the same test set)\n",
      "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test DataLoaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "                                                                                          test_dir=test_dir,\n",
    "                                                                                          transform=simple_transform,\n",
    "                                                                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create 20% training and test DataLoaders\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                                                          test_dir=test_dir,\n",
    "                                                                                          transform=simple_transform,\n",
    "                                                                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwmoMhW8IqSu"
   },
   "source": [
    "## Exercise 1: Pick a larger model from [`torchvision.models`](https://pytorch.org/vision/main/models.html) to add to the list of experiments (for example, EffNetB3 or higher)\n",
    "\n",
    "* How does it perform compared to our existing models?\n",
    "* **Hint:** You'll need to set up an exerpiment similar to [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F-35y0uxJ8tg"
   },
   "outputs": [],
   "source": [
    "#Set up a list of models we'll loop through\n",
    "models = [\"effnetb0\", \"effnetb2\", \"effnetb3\"]\n",
    "\n",
    "epochs = [5, 10]\n",
    "\n",
    "# Set up a list of dataloaders we will go through\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
    "                     \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1536, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.efficientnet_b3().classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "def create_effnetb0():\n",
    "    model = torchvision.models.efficientnet_b0(weights='DEFAULT').to(device)\n",
    "\n",
    "    # Freeze the base layer\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "    \n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "def create_effnetb2():\n",
    "    model = torchvision.models.efficientnet_b2(weights='DEFAULT').to(device)\n",
    "\n",
    "    # Freeze the base layer\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "    \n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "def create_effnetb3():\n",
    "    model = torchvision.models.efficientnet_b3(weights='DEFAULT').to(device)\n",
    "\n",
    "    # Freeze the base layer\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1536, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "    \n",
    "    model.name = \"effnetb3\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the get_state_dict method to skip has check\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.models._api import WeightsEnum\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "def get_state_dict(self, *args, **kwargs):\n",
    "    kwargs.pop(\"check_hash\")\n",
    "    return load_state_dict_from_url(self.url, *args, **kwargs)\n",
    "WeightsEnum.get_state_dict = get_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.utils import save_model\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "\n",
    "\n",
    "def create_and_train(models=List[str],\n",
    "                     epochs=List[int],\n",
    "                     data_loaders=Dict):\n",
    "    init(autoreset=True)\n",
    "    experiment_number = 0\n",
    "    \n",
    "    for dataloader_name, train_dataloader in data_loaders.items():\n",
    "        for epoch in epochs:\n",
    "            for model_name in models:\n",
    "                experiment_number +=1\n",
    "                print(Style.BRIGHT + Fore.RED + f\"Experiment Number: {experiment_number}\")\n",
    "                print(f\"Model: {model_name}\")\n",
    "                print(f\"DataLoader: {dataloader_name}\")\n",
    "                print(f\"Number of epochs: {epoch}\")\n",
    "        \n",
    "                try:\n",
    "                    assert model_name in [\"effnetb0\", \"effnetb2\", \"effnetb3\"], \"Error: Model unrecognized, defaulting to Effnetb0\"\n",
    "        \n",
    "                    if model_name == \"effnetb0\":\n",
    "                        model = create_effnetb0() \n",
    "                    elif model_name == \"effnetb2\":\n",
    "                        model = create_effnetb2()\n",
    "                    else:\n",
    "                        model = create_effnetb3()\n",
    "                except AssertionError as e:\n",
    "                    e = Style.BRIGHT + Fore.RED + e\n",
    "                    print(e)\n",
    "                    model = create_effnetb0()\n",
    "        \n",
    "                # Create optimizer and loss functions\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "                \n",
    "                train(model=model,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      test_dataloader=test_dataloader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=loss_fn,\n",
    "                      epochs=epoch,\n",
    "                      device=device,\n",
    "                      writer=create_writer(experiment_name=dataloader_name,\n",
    "                                           model_name=model_name,\n",
    "                                           extra=f\"{epoch}_epochs\"))\n",
    "        \n",
    "                # Save the model\n",
    "                save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}.pth\"\n",
    "                save_model(model=model,\n",
    "                           target_dir=\"models\",\n",
    "                           model_name=save_filepath)\n",
    "                print(Style.BRIGHT + Fore.GREEN + \"-\"*75 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqlStPo-gbrF"
   },
   "source": [
    "## Exercise 2. Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n",
    "    \n",
    "* For example, you could have one training DataLoader that uses data augmentation (e.g. `train_dataloader_20_percent_aug` and `train_dataloader_20_percent_no_aug`) and then compare the results of two of the same model types training on these two DataLoaders.\n",
    "* **Note:** You may need to alter the `create_dataloaders()` function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) for examples of using data augmentation or the script below for an example:\n",
    "\n",
    "```python\n",
    "# Note: Data augmentation transform like this should only be performed on training data\n",
    "train_transform_data_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Create a helper function to visualize different augmented (and not augmented) images\n",
    "def view_dataloader_images(dataloader, n=10):\n",
    "    if n > 10:\n",
    "        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n",
    "        n = 10\n",
    "    imgs, labels = next(iter(dataloader))\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(n):\n",
    "        # Min max scale the image for display purposes\n",
    "        targ_image = imgs[i]\n",
    "        sample_min, sample_max = targ_image.min(), targ_image.max()\n",
    "        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n",
    "\n",
    "        # Plot images with appropriate axes information\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(False)\n",
    "\n",
    "# Have to update `create_dataloaders()` to handle different augmentations\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data\n",
    "\n",
    "# Note: this is an update version of data_setup.create_dataloaders to handle\n",
    "# differnt train and test transforms.\n",
    "def create_dataloaders(\n",
    "    train_dir,\n",
    "    test_dir,\n",
    "    train_transform, # add parameter for train transform (transforms on train dataset)\n",
    "    test_transform,  # add parameter for test transform (transforms on test dataset)\n",
    "    batch_size=32, num_workers=NUM_WORKERS\n",
    "):\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "E1N3yyDOoH2t"
   },
   "outputs": [],
   "source": [
    "# Modify create_dataloader to accept two transforms\n",
    "\n",
    "\"\"\"\n",
    "Contains funcitonality for creating PyTorch DataLoader's for\n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    train_transform: transforms.Compose,\n",
    "    test_transform: transforms.Compose=torchvision.models.EfficientNet_B0_Weights.DEFAULT.transforms(),\n",
    "    batch_size: int=32,\n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "    \"\"\"Create training and testing Dataloaders\n",
    "\n",
    "    Takes in a training and testing directory path and turns them into \n",
    "    PyTorch Datasets and then into DataLoaders\n",
    "\n",
    "    Args:\n",
    "        train_dir: Path to training directory.\n",
    "        test_dir: Path to testing directory.\n",
    "        train_transform: torchvision transform to perform on training.\n",
    "        test_transform: torchvision transform to be performed on test data, default is efficientnetb0's transform\n",
    "        batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "        num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "        Where class_names is a list of the target classes.\n",
    "        Example usage:\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_transform = transforms.ToTensor()\n",
    "    \n",
    "    # Create the datasets, making sure to apply the tensor transform before any other transforms\n",
    "    train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                      transform=transforms.Compose([tensor_transform, train_transform]),\n",
    "                                      target_transform=None)\n",
    "    \n",
    "    test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                     transform=transforms.Compose([tensor_transform, test_transform]),\n",
    "                                     target_transform=None)\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True)\n",
    "\n",
    "    test_dataloader = DataLoader(dataset=test_data,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=True)\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "# Create a data augmentation transform using PyTorch's new v2 transforms\n",
    "train_transform = v2.Compose([\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create an augmented train dataloader and regular test transform\n",
    "train_dataloader_20_percent_augmented, test_dataloader_20_percent, class_names = create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                           test_dir=test_dir,\n",
    "                                                           train_transform=train_transform,\n",
    "                                                           test_transform=torchvision.models.EfficientNet_B0_Weights.DEFAULT.transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_augmented = {\"data_20_percent_augmented\": train_dataloader_20_percent_augmented,\n",
    "                               \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Number: 1\n",
      "Model: effnetb0\n",
      "DataLoader: data_20_percent_augmented\n",
      "Number of epochs: 5\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent_augmented/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2555bc31cdcd41d89c2b3fa74ab3532f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0210 | train_acc: 0.5250 | test_loss: 0.6855 | test_acc: 0.8562\n",
      "Epoch: 2 | train_loss: 0.7472 | train_acc: 0.8250 | test_loss: 0.5637 | test_acc: 0.9102\n",
      "Epoch: 3 | train_loss: 0.6180 | train_acc: 0.8479 | test_loss: 0.4706 | test_acc: 0.9006\n",
      "Epoch: 4 | train_loss: 0.5871 | train_acc: 0.8438 | test_loss: 0.4106 | test_acc: 0.9131\n",
      "Epoch: 5 | train_loss: 0.5115 | train_acc: 0.8583 | test_loss: 0.3942 | test_acc: 0.9131\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_augmented_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 2\n",
      "Model: effnetb2\n",
      "DataLoader: data_20_percent_augmented\n",
      "Number of epochs: 5\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent_augmented/effnetb2/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ab78acec3b4f7b9cd32b41c0fa663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0144 | train_acc: 0.5229 | test_loss: 0.8208 | test_acc: 0.7869\n",
      "Epoch: 2 | train_loss: 0.7980 | train_acc: 0.7000 | test_loss: 0.6738 | test_acc: 0.8449\n",
      "Epoch: 3 | train_loss: 0.6715 | train_acc: 0.8083 | test_loss: 0.5793 | test_acc: 0.8852\n",
      "Epoch: 4 | train_loss: 0.6394 | train_acc: 0.7542 | test_loss: 0.5159 | test_acc: 0.9159\n",
      "Epoch: 5 | train_loss: 0.5503 | train_acc: 0.8396 | test_loss: 0.4934 | test_acc: 0.8824\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_augmented_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 3\n",
      "Model: effnetb3\n",
      "DataLoader: data_20_percent_augmented\n",
      "Number of epochs: 5\n",
      "[INFO] Created new effnetb3 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent_augmented/effnetb3/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb16b6c031c34d7b9720b93170ff0406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9543 | train_acc: 0.6167 | test_loss: 0.8241 | test_acc: 0.8568\n",
      "Epoch: 2 | train_loss: 0.7977 | train_acc: 0.7354 | test_loss: 0.6325 | test_acc: 0.8892\n",
      "Epoch: 3 | train_loss: 0.6646 | train_acc: 0.7500 | test_loss: 0.5289 | test_acc: 0.9506\n",
      "Epoch: 4 | train_loss: 0.5970 | train_acc: 0.8396 | test_loss: 0.4948 | test_acc: 0.9227\n",
      "Epoch: 5 | train_loss: 0.5522 | train_acc: 0.8375 | test_loss: 0.4423 | test_acc: 0.9381\n",
      "[INFO] Saving model to: models/07_effnetb3_data_20_percent_augmented_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 4\n",
      "Model: effnetb0\n",
      "DataLoader: data_20_percent_augmented\n",
      "Number of epochs: 10\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent_augmented/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c2a46ab1c04de8981f40d358738884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9281 | train_acc: 0.6417 | test_loss: 0.6471 | test_acc: 0.8670\n",
      "Epoch: 2 | train_loss: 0.7038 | train_acc: 0.8167 | test_loss: 0.5155 | test_acc: 0.8949\n",
      "Epoch: 3 | train_loss: 0.6155 | train_acc: 0.8354 | test_loss: 0.4163 | test_acc: 0.9222\n",
      "Epoch: 4 | train_loss: 0.5853 | train_acc: 0.7792 | test_loss: 0.3866 | test_acc: 0.9159\n",
      "Epoch: 5 | train_loss: 0.4889 | train_acc: 0.8625 | test_loss: 0.3674 | test_acc: 0.9074\n",
      "Epoch: 6 | train_loss: 0.4950 | train_acc: 0.8083 | test_loss: 0.3328 | test_acc: 0.9222\n",
      "Epoch: 7 | train_loss: 0.4818 | train_acc: 0.8396 | test_loss: 0.3315 | test_acc: 0.9256\n",
      "Epoch: 8 | train_loss: 0.4726 | train_acc: 0.8646 | test_loss: 0.3271 | test_acc: 0.9193\n",
      "Epoch: 9 | train_loss: 0.4290 | train_acc: 0.8583 | test_loss: 0.3036 | test_acc: 0.9347\n",
      "Epoch: 10 | train_loss: 0.4070 | train_acc: 0.8938 | test_loss: 0.3081 | test_acc: 0.9136\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_augmented_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 5\n",
      "Model: effnetb2\n",
      "DataLoader: data_20_percent_augmented\n",
      "Number of epochs: 10\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent_augmented/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3702963bb0542c98bcfabb3172f0892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0060 | train_acc: 0.5021 | test_loss: 0.7921 | test_acc: 0.8398\n",
      "Epoch: 2 | train_loss: 0.7874 | train_acc: 0.7729 | test_loss: 0.6715 | test_acc: 0.8733\n",
      "Epoch: 3 | train_loss: 0.6690 | train_acc: 0.7875 | test_loss: 0.5485 | test_acc: 0.8886\n",
      "Epoch: 4 | train_loss: 0.6352 | train_acc: 0.7729 | test_loss: 0.5189 | test_acc: 0.8920\n",
      "Epoch: 5 | train_loss: 0.5843 | train_acc: 0.8000 | test_loss: 0.4802 | test_acc: 0.8949\n",
      "Epoch: 6 | train_loss: 0.5236 | train_acc: 0.8187 | test_loss: 0.4565 | test_acc: 0.9074\n",
      "Epoch: 7 | train_loss: 0.4880 | train_acc: 0.8708 | test_loss: 0.4308 | test_acc: 0.9074\n",
      "Epoch: 8 | train_loss: 0.5418 | train_acc: 0.7854 | test_loss: 0.4194 | test_acc: 0.9136\n",
      "Epoch: 9 | train_loss: 0.4831 | train_acc: 0.8542 | test_loss: 0.4170 | test_acc: 0.9011\n",
      "Epoch: 10 | train_loss: 0.4581 | train_acc: 0.8604 | test_loss: 0.4140 | test_acc: 0.8949\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_augmented_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 6\n",
      "Model: effnetb3\n",
      "DataLoader: data_20_percent_augmented\n",
      "Number of epochs: 10\n",
      "[INFO] Created new effnetb3 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent_augmented/effnetb3/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b03e12a45f743ca9fe250711c31f8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9824 | train_acc: 0.5708 | test_loss: 0.7936 | test_acc: 0.8540\n",
      "Epoch: 2 | train_loss: 0.7418 | train_acc: 0.8354 | test_loss: 0.6379 | test_acc: 0.8795\n",
      "Epoch: 3 | train_loss: 0.6820 | train_acc: 0.8000 | test_loss: 0.5356 | test_acc: 0.8955\n",
      "Epoch: 4 | train_loss: 0.6135 | train_acc: 0.7854 | test_loss: 0.4912 | test_acc: 0.9074\n",
      "Epoch: 5 | train_loss: 0.5777 | train_acc: 0.8167 | test_loss: 0.4566 | test_acc: 0.8926\n",
      "Epoch: 6 | train_loss: 0.5200 | train_acc: 0.8333 | test_loss: 0.4207 | test_acc: 0.9199\n",
      "Epoch: 7 | train_loss: 0.5333 | train_acc: 0.8354 | test_loss: 0.4086 | test_acc: 0.9170\n",
      "Epoch: 8 | train_loss: 0.5173 | train_acc: 0.8187 | test_loss: 0.3981 | test_acc: 0.9193\n",
      "Epoch: 9 | train_loss: 0.4782 | train_acc: 0.8625 | test_loss: 0.3823 | test_acc: 0.9318\n",
      "Epoch: 10 | train_loss: 0.4527 | train_acc: 0.8688 | test_loss: 0.3740 | test_acc: 0.9352\n",
      "[INFO] Saving model to: models/07_effnetb3_data_20_percent_augmented_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 7\n",
      "Model: effnetb0\n",
      "DataLoader: data_20_percent\n",
      "Number of epochs: 5\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d615144d2140fca0004b06564b586e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9802 | train_acc: 0.5333 | test_loss: 0.6802 | test_acc: 0.8636\n",
      "Epoch: 2 | train_loss: 0.7102 | train_acc: 0.8229 | test_loss: 0.5638 | test_acc: 0.9006\n",
      "Epoch: 3 | train_loss: 0.6009 | train_acc: 0.8167 | test_loss: 0.4853 | test_acc: 0.8915\n",
      "Epoch: 4 | train_loss: 0.5250 | train_acc: 0.8063 | test_loss: 0.4300 | test_acc: 0.8943\n",
      "Epoch: 5 | train_loss: 0.4909 | train_acc: 0.8146 | test_loss: 0.4094 | test_acc: 0.8915\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 8\n",
      "Model: effnetb2\n",
      "DataLoader: data_20_percent\n",
      "Number of epochs: 5\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent/effnetb2/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b205913f2cf54d7fb1e65a8a96e01fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9975 | train_acc: 0.5146 | test_loss: 0.7642 | test_acc: 0.8489\n",
      "Epoch: 2 | train_loss: 0.7874 | train_acc: 0.7583 | test_loss: 0.6540 | test_acc: 0.8602\n",
      "Epoch: 3 | train_loss: 0.6326 | train_acc: 0.8208 | test_loss: 0.6069 | test_acc: 0.8364\n",
      "Epoch: 4 | train_loss: 0.5385 | train_acc: 0.8562 | test_loss: 0.5427 | test_acc: 0.8824\n",
      "Epoch: 5 | train_loss: 0.4898 | train_acc: 0.8750 | test_loss: 0.5084 | test_acc: 0.8943\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 9\n",
      "Model: effnetb3\n",
      "DataLoader: data_20_percent\n",
      "Number of epochs: 5\n",
      "[INFO] Created new effnetb3 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent/effnetb3/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8895febaecae48ca8e59dfb49c89ff54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9613 | train_acc: 0.6104 | test_loss: 0.8124 | test_acc: 0.8795\n",
      "Epoch: 2 | train_loss: 0.7377 | train_acc: 0.8313 | test_loss: 0.6229 | test_acc: 0.8580\n",
      "Epoch: 3 | train_loss: 0.5621 | train_acc: 0.8958 | test_loss: 0.5174 | test_acc: 0.9045\n",
      "Epoch: 4 | train_loss: 0.5579 | train_acc: 0.8458 | test_loss: 0.4683 | test_acc: 0.9170\n",
      "Epoch: 5 | train_loss: 0.5194 | train_acc: 0.8771 | test_loss: 0.4727 | test_acc: 0.8795\n",
      "[INFO] Saving model to: models/07_effnetb3_data_20_percent_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 10\n",
      "Model: effnetb0\n",
      "DataLoader: data_20_percent\n",
      "Number of epochs: 10\n",
      "[INFO] Created new effnetb0 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c867244b8a2f41008c62dcafb17e8fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9221 | train_acc: 0.6312 | test_loss: 0.6199 | test_acc: 0.9097\n",
      "Epoch: 2 | train_loss: 0.6834 | train_acc: 0.8042 | test_loss: 0.5188 | test_acc: 0.8972\n",
      "Epoch: 3 | train_loss: 0.5273 | train_acc: 0.8667 | test_loss: 0.4387 | test_acc: 0.9034\n",
      "Epoch: 4 | train_loss: 0.4565 | train_acc: 0.9062 | test_loss: 0.4177 | test_acc: 0.9068\n",
      "Epoch: 5 | train_loss: 0.4704 | train_acc: 0.8625 | test_loss: 0.3766 | test_acc: 0.9131\n",
      "Epoch: 6 | train_loss: 0.4397 | train_acc: 0.8708 | test_loss: 0.3585 | test_acc: 0.9131\n",
      "Epoch: 7 | train_loss: 0.3698 | train_acc: 0.9083 | test_loss: 0.3516 | test_acc: 0.9068\n",
      "Epoch: 8 | train_loss: 0.3451 | train_acc: 0.9250 | test_loss: 0.3153 | test_acc: 0.9159\n",
      "Epoch: 9 | train_loss: 0.3330 | train_acc: 0.9292 | test_loss: 0.2995 | test_acc: 0.9409\n",
      "Epoch: 10 | train_loss: 0.3769 | train_acc: 0.8562 | test_loss: 0.3109 | test_acc: 0.9284\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 11\n",
      "Model: effnetb2\n",
      "DataLoader: data_20_percent\n",
      "Number of epochs: 10\n",
      "[INFO] Created new effnetb2 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c38c82edea40b3b8aa4b1a9c2c07c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9910 | train_acc: 0.5583 | test_loss: 0.7945 | test_acc: 0.8545\n",
      "Epoch: 2 | train_loss: 0.7317 | train_acc: 0.8521 | test_loss: 0.6343 | test_acc: 0.8665\n",
      "Epoch: 3 | train_loss: 0.5862 | train_acc: 0.8542 | test_loss: 0.5694 | test_acc: 0.9102\n",
      "Epoch: 4 | train_loss: 0.4894 | train_acc: 0.9146 | test_loss: 0.5190 | test_acc: 0.8977\n",
      "Epoch: 5 | train_loss: 0.4465 | train_acc: 0.9042 | test_loss: 0.4743 | test_acc: 0.9040\n",
      "Epoch: 6 | train_loss: 0.4213 | train_acc: 0.9021 | test_loss: 0.4532 | test_acc: 0.9102\n",
      "Epoch: 7 | train_loss: 0.3883 | train_acc: 0.9125 | test_loss: 0.4343 | test_acc: 0.8886\n",
      "Epoch: 8 | train_loss: 0.3672 | train_acc: 0.9042 | test_loss: 0.4265 | test_acc: 0.9011\n",
      "Epoch: 9 | train_loss: 0.3703 | train_acc: 0.8958 | test_loss: 0.4077 | test_acc: 0.8761\n",
      "Epoch: 10 | train_loss: 0.3361 | train_acc: 0.9479 | test_loss: 0.4002 | test_acc: 0.8886\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Experiment Number: 12\n",
      "Model: effnetb3\n",
      "DataLoader: data_20_percent\n",
      "Number of epochs: 10\n",
      "[INFO] Created new effnetb3 model.\n",
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/data_20_percent/effnetb3/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff597e88bd54889b5bec56d43294522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9895 | train_acc: 0.5646 | test_loss: 0.8040 | test_acc: 0.8824\n",
      "Epoch: 2 | train_loss: 0.7518 | train_acc: 0.7625 | test_loss: 0.6239 | test_acc: 0.9324\n",
      "Epoch: 3 | train_loss: 0.5758 | train_acc: 0.9042 | test_loss: 0.5383 | test_acc: 0.8761\n",
      "Epoch: 4 | train_loss: 0.5028 | train_acc: 0.9062 | test_loss: 0.4816 | test_acc: 0.9074\n",
      "Epoch: 5 | train_loss: 0.4783 | train_acc: 0.8812 | test_loss: 0.4439 | test_acc: 0.9045\n",
      "Epoch: 6 | train_loss: 0.5013 | train_acc: 0.8167 | test_loss: 0.4222 | test_acc: 0.8920\n",
      "Epoch: 7 | train_loss: 0.4372 | train_acc: 0.8604 | test_loss: 0.3898 | test_acc: 0.9261\n",
      "Epoch: 8 | train_loss: 0.3800 | train_acc: 0.9125 | test_loss: 0.3842 | test_acc: 0.9386\n",
      "Epoch: 9 | train_loss: 0.4204 | train_acc: 0.8750 | test_loss: 0.3709 | test_acc: 0.9324\n",
      "Epoch: 10 | train_loss: 0.3918 | train_acc: 0.8625 | test_loss: 0.3643 | test_acc: 0.9136\n",
      "[INFO] Saving model to: models/07_effnetb3_data_20_percent_[5, 10].pth\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "CPU times: user 1min 23s, sys: 46.8 s, total: 2min 10s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "create_and_train(models=models,\n",
    "                 epochs=epochs,\n",
    "                 data_loaders=train_dataloaders_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IvuTskxgjaw"
   },
   "source": [
    "## Exercise 3. Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire [Food101 dataset from `torchvision.models`](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101)\n",
    "    \n",
    "* You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\n",
    "* If you try more than one model, it would be good to have the model's results tracked.\n",
    "* If you load the Food101 dataset from `torchvision.models`, you'll have to create PyTorch DataLoaders to use it in training.\n",
    "* **Note:** Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YehliYnYoP1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 4996278331/4996278331 [07:51<00:00, 10595367.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/food-101.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "# Create transforms we will use\n",
    "transform_v2m = torchvision.models.EfficientNet_V2_M_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Download the data\n",
    "train_data = torchvision.datasets.Food101(root=\"data\",\n",
    "                                          split=\"train\",\n",
    "                                          transform=transform_v2m,\n",
    "                                          download=False)\n",
    "\n",
    "test_data = torchvision.datasets.Food101(root=\"data\",\n",
    "                                          split=\"test\",\n",
    "                                          transform=transform_v2m,\n",
    "                                          download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[480]\n",
       "    resize_size=[480]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_v2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FEATURES = len(train_data.classes)\n",
    "\n",
    "# Create a model\n",
    "model_v2m = torchvision.models.efficientnet_v2_m(weights='DEFAULT').to(device)\n",
    "\n",
    "# Freeze the base layer\n",
    "for param in model_v2m.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Change the classifier head of the model\n",
    "model_v2m.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model=model_v2m,\n",
    "        input_size=(1, 3, 480, 480),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "import os\n",
    "\n",
    "train_dataloader_big = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                                   shuffle=True,\n",
    "                                                   pin_memory=True,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=os.cpu_count())\n",
    "\n",
    "test_dataloader_big = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                                   shuffle=False,\n",
    "                                                   pin_memory=True,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: runs/2023-10-25/food101_all_data/foodvision_big/[5, 10]_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101b07360f744d40983ac50c74e7739e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m foodvision_big_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_v2m\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader_big\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader_big\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_v2m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfood101_all_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfoodvision_big\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, writer)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m     60\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     61\u001b[0m                                       dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     62\u001b[0m                                       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     63\u001b[0m                                       optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     64\u001b[0m                                       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 65\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     72\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/PyTorch/Code/going_modular/engine.py:106\u001b[0m, in \u001b[0;36mtest_step\u001b[0;34m(model, dataloader, loss_fn, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 2. Calculate and accumulate loss\u001b[39;00m\n\u001b[1;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(test_pred_logits, y)\n\u001b[0;32m--> 106\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Calculate and accumulate accuracy\u001b[39;00m\n\u001b[1;32m    109\u001b[0m test_pred_labels \u001b[38;5;241m=\u001b[39m test_pred_logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "foodvision_big_results = train(model=model_v2m,\n",
    "                               train_dataloader=train_dataloader_big,\n",
    "                               test_dataloader=test_dataloader_big,\n",
    "                               optimizer=torch.optim.Adam(params=model_v2m.parameters(), lr=0.001),\n",
    "                               loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                               epochs=5,\n",
    "                               device=device,\n",
    "                               writer=create_writer(experiment_name=\"food101_all_data\",\n",
    "                                                    model_name=\"foodvision_big\",\n",
    "                                                    extra=f\"{epochs}_epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "07_pytorch_experiment_tracking_exercise_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
